{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from bidict import bidict\n",
    "from collections import defaultdict\n",
    "\n",
    "from jpeg123 import *\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huffman Codes\n",
    "\n",
    "Invented in 1952 by David Huffman, a graduate student at MIT. They are one type of optimal prefix-free codes used for lossless compression. Huffman trees are constructed using the entropy of each \"word\" that we want to be able to transmit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "### What is entropy?\n",
    "$$H(p) = -p\\log_2(p)$$\n",
    "$$H(\\mathcal{D}(x)) = \\sum_{x\\in \\mathcal{X}} -p(x) \\log_2(p(x))$$\n",
    "* Intuition: higher entropy means less knowledge about the distribution.\n",
    "* Intuition: higher entropy of a symbol means you learned more when you received it\n",
    "\n",
    "Which binomial distribution $\\mathcal{B}(p)$ has higher entropy, \n",
    "$$\\mathcal{B}(0.5): P(0)=P(1)=0.5$$ \n",
    "or \n",
    "$$\\mathcal{B}(0.1): P(1)=0.1,\\, P(0)=0.9?$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$H(\\mathcal{B}(0.5)) = -0.5\\log_2(0.5)\\cdot 2 = 1$$\n",
    "$$H(\\mathcal{B}(0.1)) = -0.1\\log_2(0.1) + -0.9\\log_2(0.9) = 0.469\\ldots$$\n",
    "\n",
    "What is $H(\\mathcal{B}(0.9))$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binomial(p):\n",
    "    \"\"\"Simple binomial distribution probabilities\"\"\"\n",
    "    return np.array([p, 1-p])\n",
    "\n",
    "def Hp(p):\n",
    "    \"\"\"Entropy of a probability p\"\"\"\n",
    "    return -p * np.log2(p)\n",
    "\n",
    "def Hdist(probs):\n",
    "    \"\"\"\n",
    "    Entropy of a probability distribution\n",
    "    probs: np.array of sample probabilities\n",
    "    \"\"\"\n",
    "    return sum(Hp(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hdist(binomial(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hdist(binomial(0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = np.arange(0.01, 1, 0.01)\n",
    "entropies = [Hdist(binomial(p)) for p in probs]\n",
    "fig = plt.figure(figsize=(12,7))\n",
    "plt.plot(probs, entropies)\n",
    "plt.xlabel(\"p\")\n",
    "plt.ylabel(\"H(B(p))\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why do we care about the entropy of a distribution?\n",
    "It turns out the entropy of a distribution represents the **minimum** bits per value it takes to send a message from the distribution on average. _Achieving_ that minimum can be very tricky.\n",
    "\n",
    "Imagine we have a message we want to send, written in English. In this message, each word occurs with some frequency, and further each letter also occurs with some frequency.\n",
    "\n",
    "Since the message is written in English, we have some _a priori_ knowledge about which words exist and how often they occur, and the same for letters.\n",
    "\n",
    "There's an XKCD for that: https://xkcd.com/simplewriter/ and https://xkcd.com/1133"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message1 = \"the quick brown fox jumps over the lazy dog\"\n",
    "message2 = \"a baby bat beats the short cat to the table\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://web.archive.org/web/20170918020907/http://www.data-compression.com/english.html\n",
    "letters = list(map(lambda x: chr(x), list(ord(\"A\") + np.arange(26)) + [ord(\" \")]))\n",
    "freqs = [0.0651738, 0.0124248, 0.0217339, 0.0349835, 0.1041442, 0.0197881, 0.0158610, 0.0492888, 0.0558094, 0.0009033, 0.0050529, 0.0331490, 0.0202124, 0.0564513, 0.0596302, 0.0137645, 0.0008606, 0.0497563, 0.0515760, 0.0729357, 0.0225134, 0.0082903, 0.0171272, 0.0013692, 0.0145984, 0.0007836, 0.1918182]\n",
    "letter_freqs = {l: f for l, f in zip(letters, freqs)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,7))\n",
    "plt.bar(letter_freqs.keys(), letter_freqs.values())\n",
    "plt.xlabel(\"Letter\")\n",
    "plt.ylabel(\"Relative Frequency (Q=1)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to send our message using the smallest amount of bits possible, we have many options for representing the message.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom fixed-size codes\n",
    "First we could assign every English word a number, then transmit the sequence of numbers. There are ~171,000 English words, so we need at least 18 bits per number to represent every word ($2^{18} = 262,144 > 171,000$).\n",
    "\n",
    "There are 9 words in the first message, so we need **162** bits to send the message with this encoding.\n",
    "\n",
    "There are 10 words in the second, so we need **180** bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(2 ** 18)\n",
    "print(len(message1.split()) * 18)\n",
    "print(len(message2.split()) * 18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could use the 8-bit ASCII representation of each letter (plus space).\n",
    "\n",
    "This results in **344** bits for both messages, since they are the same length. It turns out this is worse than enumerating every english word!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(message1) * 8)\n",
    "print(len(message2) * 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do better though, since we only care about the 26 lower case letters and space. This requires 5 bits per character, resulting in **215** bits per message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(message1) * 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this the best we can do though? The entropy of English letters is 4.07, so in theory we should only need just over 4 bits per letter. This would be **176** bits per message on average with a good compression scheme.\n",
    "\n",
    "##### Food for thought:\n",
    "Why isn't this lower than our minimum when enumerating every word?\n",
    "\n",
    "What other information about letters could we take advantage of?\n",
    "\n",
    "With only what we know now, how can we improve our per-letter encoding scheme?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "letter_probs = np.array(list(letter_freqs.values()))\n",
    "letter_entropy = Hdist(letter_probs)\n",
    "print(\"English letter entropy:\", letter_entropy)\n",
    "print(\"Average bits per message:\", letter_entropy * len(message1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prefix-free codes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we allow the encoded version of every letter, or codeword, to vary in length? We can use shorter codewords for common letters and longer codewords for rarer letters.\n",
    "\n",
    "In order to uniquely decode every encoded message we need a way to distinguish codeword boundaries. One way to accomplish this is prefix-free coding, in which no codeword can be the prefix of any other. A natural representation for these codes is a binary tree. Each left branch adds a 0 to the codeword and each right branch a 1. Tracing branches from the root to the leaf representing your desired symbol automatically generates your codeword for that symbol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huffman's Code Construction Algorithm\n",
    "###### Adapted from https://math.mit.edu/~goemans/18310S15/huffman-notes.pdf\n",
    "\n",
    "Let's illustrate the construction of a Huffman code with a small alphabet:\n",
    "* **a**, $p(a)=0.4$\n",
    "* **b**, $p(b)=0.05$\n",
    "* **c**, $p(c)=0.18$\n",
    "* **d**, $p(d)=0.07$\n",
    "* **e**, $p(e)=0.1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, assume that d will be assigned the longest codeword because it has the lowest probability.\n",
    "\n",
    "_Argue that if another letter has a longer codeword, we can swap with d and have better compression on average._\n",
    "\n",
    "It follows that b will share the longest codeword length with d. Now we know our prefix code's tree representation has the subtree:\n",
    "\n",
    "![subtree bd](./bdtree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can replace the b-d subtree with a placeholder letter $\\alpha$, which has associated probability \n",
    "$$p(\\alpha) = p(b) + p(d) = 0.12.$$\n",
    "\n",
    "Next, we repeat the reasoning from the previous step to construct a new subtree with the next two lowest probability letters, f ($p=0.1$) and $\\alpha$ ($p=0.12$).\n",
    "\n",
    "![falpha](falphatree.png)\n",
    "\n",
    "Expanding $\\alpha$, we get the partial prefix code\n",
    "\n",
    "![fbd](fbdtree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregating f and $\\alpha$ into $\\beta$, $p(\\beta)=0.22$ we continue this reverse tree construction. \n",
    "\n",
    "The next two letters to be chosen are actually c and e, creating $\\gamma$, $p(\\gamma)=0.38$.\n",
    "\n",
    "We choose $\\beta$ and $\\gamma$ to create $\\delta$, $p(\\delta)=0.6$. Now we are left with only a and $\\delta$, and we have finally reached the root of the code tree.\n",
    "\n",
    "![adelta](adeltatree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expand the each intermediate letter in the tree to achieve our final code.\n",
    "![fulltree](fulltree.png)\n",
    "![codewords](codewords.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are there any prefix codewords?\n",
    "\n",
    "What is the expected number of bits per letter, and how does this compare to the entropy of our alphabet?\n",
    "$$\\mathbb{E}[\\text{bits/letter}] = \\sum_l p(l)\\cdot \\text{len}(l)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_bits = 1 * 0.4 + 4 * 0.05 + 3 * 0.18 + 4 * 0.07 + 3 * 0.2 + 3 * 0.1\n",
    "print(\"E[b/l] =\", expected_bits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy_alphabet = Hdist(np.array([0.4, 0.05, 0.18, 0.07, 0.2, 0.1]))\n",
    "print(\"H =\", entropy_alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very close to the entropy lower bound!\n",
    "\n",
    "It turns out that, due to codewords being a whole number of bits, Huffman coding gives an expected length $L$ per letter\n",
    "$$H(p) \\leq L \\leq H(p) + 1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it together\n",
    "Let's turn this algorithm into code.\n",
    "\n",
    "First, are there any useful datastructures for storing our alphabet as it changes?\n",
    "\n",
    "How will we store the tree as we build it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, p, left, right=None):\n",
    "        self.p = p\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        # depth ensures the algorithm prefers to combine shallow\n",
    "        # trees when selected items of equal probability\n",
    "        if right is None:\n",
    "            self.depth = 1\n",
    "        else:\n",
    "            self.depth = 1 + max(self.left.depth, self.right.depth)\n",
    "\n",
    "    def __lt__(self, other):\n",
    "        \"\"\"\n",
    "        Compare two tree nodes, sorting first by probability then\n",
    "        by depth of tree. This is the low-level function called\n",
    "        by min or the less-than operator when the arguments are\n",
    "        instances of Node.\n",
    "        \"\"\"\n",
    "        return (self.p < other.p or\n",
    "                (self.p == other.p and self.depth < other.depth))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        if self.isLeaf():\n",
    "            depth_or_leaf = \"letter=%s\" % str(self.left)\n",
    "        else:\n",
    "            depth_or_leaf = \"depth=%d\" % self.depth\n",
    "        return \"Node(p=%.4f, %s)\" % (self.p, depth_or_leaf)\n",
    "    \n",
    "    def isLeaf(self):\n",
    "        return self.depth == 1\n",
    "    \n",
    "    def walk(self, code_dict, prefix=\"\"):\n",
    "        \"\"\"\n",
    "        Recursive tree-walk to construct the prefix-free code by finding\n",
    "        all leaves.\n",
    "        \"\"\"\n",
    "        if self.isLeaf():\n",
    "            code_dict[self.left] = prefix\n",
    "        else:\n",
    "            self.left.walk(code_dict, prefix + \"0\")\n",
    "            self.right.walk(code_dict, prefix + \"1\")\n",
    "        \n",
    "\n",
    "def dict_to_alphabet(d):\n",
    "    \"\"\"\n",
    "    Takes a dictionary of symbol=>probability pairs and converts them \n",
    "    to our alphabet representation for use in tree construction.\n",
    "    Basically, initializes all the leaves of the tree.\n",
    "    \"\"\"\n",
    "    pairs = [(v, k) for (k, v) in d.items()]\n",
    "    nodes = [Node(p, letter) for (p, letter) in pairs]\n",
    "    # Note: python compares tuples in order of the entries\n",
    "    heapq.heapify(nodes)\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = dict_to_alphabet(letter_freqs)\n",
    "print(alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's code up our recursive longest-subtree operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tree(alphabet):\n",
    "    \"\"\"\n",
    "    Pop the two least-probable letters in our alphabet and create the parent/merged letter.\n",
    "    Recurse until we run out of letters.\n",
    "    \n",
    "    Returns the root node of the tree\n",
    "    \"\"\"\n",
    "    # Get the two smallest probability letters\n",
    "    right = heapq.heappop(alphabet)\n",
    "    left = heapq.heappop(alphabet)\n",
    "    # Create the parent node\n",
    "    parent = Node(right.p + left.p, left, right)\n",
    "    # Push the parent node onto the heap (like alpha in the example)\n",
    "    heapq.heappush(alphabet, parent)\n",
    "    # Check if we've reached the root node, otherwise recurse\n",
    "    if len(alphabet) == 1:\n",
    "        return alphabet[0]\n",
    "    else:\n",
    "        return build_tree(alphabet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's test it!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_dict = {\n",
    "    \"a\": 0.4,\n",
    "    \"b\": 0.05,\n",
    "    \"c\": 0.18,\n",
    "    \"d\": 0.07,\n",
    "    \"e\": 0.2,\n",
    "    \"f\": 0.1,\n",
    "}\n",
    "\n",
    "# Form the leaves\n",
    "alphabet = dict_to_alphabet(example_dict)\n",
    "print(alphabet)\n",
    "print()\n",
    "\n",
    "# Construct the full tree\n",
    "root = build_tree(alphabet)\n",
    "print(root)\n",
    "print()\n",
    "\n",
    "# Now turn the tree into a prefix-free code\n",
    "encoding_dict = bidict()\n",
    "root.walk(encoding_dict)\n",
    "print(\"Code:\")\n",
    "for k, v in encoding_dict.items():\n",
    "    print(\"\\t%s: %s\" % (k, v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does our code perform on English letters? Let's calculate the expected bits per letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(message, encoding):\n",
    "    return \"\".join(encoding[l] for l in message)\n",
    "    \n",
    "def expected_bits_per_letter(encoding, frequencies):\n",
    "    expectation = 0\n",
    "    for letter, codeword in encoding.items():\n",
    "        prob = frequencies[letter]\n",
    "        expectation += prob * len(codeword)\n",
    "    return expectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = dict_to_alphabet(letter_freqs)\n",
    "root = build_tree(alphabet)\n",
    "letter_encoding = bidict()\n",
    "root.walk(letter_encoding)\n",
    "exp_len = expected_bits_per_letter(letter_encoding, letter_freqs)\n",
    "print(\"Expected bits per letter:\", exp_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message1_encoded = encode(message1.upper(), letter_encoding)\n",
    "print(\"message1:\", message1)\n",
    "print(\"C(message1):\", message1_encoded)\n",
    "print(\"encoded length:\", len(message1_encoded))\n",
    "print(\"bits per letter:\", len(message1_encoded) / len(message1))\n",
    "print()\n",
    "message2_encoded = encode(message2.upper(), letter_encoding)\n",
    "print(\"message2:\", message2)\n",
    "print(\"C(message2):\", message2_encoded)\n",
    "print(\"encoded length:\", len(message2_encoded))\n",
    "print(\"bits per letter:\", len(message2_encoded) / len(message2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very close to the entropy of 4.07!\n",
    "\n",
    "We can see how messages with lots of rare letters like q, j, z take more bits to encode than messages with common letters like a, t, s.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoding\n",
    "Let's make sure we can decode our encode messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(message, encoding):\n",
    "    output = []\n",
    "    prefix_size = 1\n",
    "    while len(message) > 0:\n",
    "        # Sanity check\n",
    "        assert prefix_size <= len(message)\n",
    "        # Check if our current prefix is a valid codeword\n",
    "        maybe_cw = message[:prefix_size]\n",
    "        if maybe_cw in encoding.inverse:\n",
    "            # Valid: output the decoded letter, remove the prefix from the message, \n",
    "            # and reset the prefix size\n",
    "            output.append(encoding.inverse[maybe_cw])\n",
    "            message = message[prefix_size:]\n",
    "            prefix_size = 1\n",
    "        else:\n",
    "            # We don't have a valid codeword yet, increase the prefix size\n",
    "            prefix_size += 1\n",
    "    return output\n",
    "\n",
    "def decode_str(message, encoding):\n",
    "    return \"\".join(decode(message, encoding))\n",
    "\n",
    "def decode_generator(message_encoding):\n",
    "    # Yielding generator form of decoder for use with jpeg123\n",
    "    prefix_size = 1\n",
    "    while len(message) > 0:\n",
    "        # Sanity check\n",
    "        assert prefix_size <= len(message)\n",
    "        # Check if our current prefix is a valid codeword\n",
    "        maybe_cw = message[:prefix_size]\n",
    "        if maybe_cw in encoding.inverse:\n",
    "            # Valid: output the decoded letter, remove the prefix from the message, \n",
    "            # and reset the prefix size\n",
    "            yield encoding.inverse[maybe_cw]\n",
    "            message = message[prefix_size:]\n",
    "            prefix_size = 1\n",
    "        else:\n",
    "            # We don't have a valid codeword yet, increase the prefix size\n",
    "            prefix_size += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_message1 = decode_str(message1_encoded, letter_encoding)\n",
    "print(\"message1   :\", message1)\n",
    "print(\"message1(?):\", maybe_message1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message-specific Huffman codes\n",
    "Can we do better for either message? What if we construct a Huffman code unique to each message?\n",
    "\n",
    "First, we need to know the letter frequencies for each message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_letter_frequencies(message):\n",
    "    # Get occurence count of each letter in the message\n",
    "    counts = defaultdict(lambda: 0)\n",
    "    for l in message:\n",
    "        counts[l] += 1\n",
    "    # Convert counts to probabilities\n",
    "    num_letters = len(message)\n",
    "    for l in counts:\n",
    "        counts[l] /= num_letters\n",
    "    return dict(counts)\n",
    "\n",
    "message1_freqs = calc_letter_frequencies(message1)\n",
    "message2_freqs = calc_letter_frequencies(message2)\n",
    "print(message1_freqs)\n",
    "print(\"Unique letters:\", len(message1_freqs))\n",
    "print()\n",
    "print(message2_freqs)\n",
    "print(\"Unique letters:\", len(message2_freqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we can create custom Huffman codes and compare encoded lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = dict_to_alphabet(message1_freqs)\n",
    "root = build_tree(alphabet)\n",
    "letter_encoding = bidict()\n",
    "root.walk(letter_encoding)\n",
    "exp_len = expected_bits_per_letter(letter_encoding, message1_freqs)\n",
    "message1_custom_encoded = \"\".join([letter_encoding[l] for l in message1])\n",
    "print(\"Total length:\", len(message1_custom_encoded))\n",
    "print(\"Expected bits per letter:\", exp_len)\n",
    "print(\"Bits per letter:\\t\", len(message1_custom_encoded) / len(message1))\n",
    "print(\"Entropy:\\t\\t\", Hdist(np.array(list(message1_freqs.values()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = dict_to_alphabet(message2_freqs)\n",
    "root = build_tree(alphabet)\n",
    "letter_encoding = bidict()\n",
    "root.walk(letter_encoding)\n",
    "exp_len = expected_bits_per_letter(letter_encoding, message2_freqs)\n",
    "message2_custom_encoded = \"\".join([letter_encoding[l] for l in message2])\n",
    "print(\"Total length:\", len(message2_custom_encoded))\n",
    "print(\"Expected bits per letter:\", exp_len)\n",
    "print(\"Bits per letter:\\t\", len(message2_custom_encoded) / len(message2))\n",
    "print(\"Entropy:\\t\\t\", Hdist(np.array(list(message2_freqs.values()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better! However, in order for a friend to decode the message they need to know what our code tree looks like. There are a few ways we can achieve this.\n",
    "\n",
    "One option is to somehow send them our tree beforehand, like we do for the jpeg123 implementation with pre-chosen Huffman codes.\n",
    "\n",
    "Another option is to choose a representation for the tree structure and send it along with the rest of our message. This adds overhead to our compression scheme, but if the main body of our message is much bigger than the tree representation we can still achieve good compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The tree representation is left up to you for the final project! Experiment and see how small you can make the representation while including all necessary information.\n",
    "\n",
    "Try converting your code to a canonical Huffman code (see Wikipedia) and think about how you can serialize only the minimum information needed to reconstruct the full code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Huffman Codes for JPEG123\n",
    "\n",
    "Let's explore the potential compression gains from custom Huffman codes for your jpeg123 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_code_dict(freq_table):\n",
    "    alphabet = dict_to_alphabet(freq_table)\n",
    "    root = build_tree(alphabet)\n",
    "    code_dict = bidict()\n",
    "    root.walk(code_dict)\n",
    "    print(\"Expected bits/symbol:\", expected_bits_per_letter(code_dict, freq_table))\n",
    "    return code_dict\n",
    "\n",
    "def get_entropy(freq_table):\n",
    "    return Hdist(np.array(list(freq_table.values())))\n",
    "\n",
    "def custom_encode_image(img, quality=75, plot_freqs=False):\n",
    "    # Perform compression up to Huffman encoding\n",
    "    ydc, yac, cbdc, cbac, crdc, crac = process_image(img, quality=quality)\n",
    "    # Build the coefficient frequency tables\n",
    "    dc_freqs = calc_letter_frequencies(ydc + cbdc + crdc)\n",
    "    ac_freqs = calc_letter_frequencies(yac + cbac + crac)\n",
    "    if plot_freqs:\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        plt.bar(dc_freqs.keys(), dc_freqs.values())\n",
    "        plt.title(\"DC coefficient probabilities\")\n",
    "        plt.show()\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        plt.bar(list(map(str, ac_freqs.keys())), ac_freqs.values())\n",
    "        plt.title(\"AC coefficient probabilities\")\n",
    "        plt.show()\n",
    "    # Generate Huffman code dictionaries\n",
    "    dc_code = get_code_dict(dc_freqs)\n",
    "    ac_code = get_code_dict(ac_freqs)\n",
    "    # Encode coefficients\n",
    "    # DC\n",
    "    ydc_c = encode(ydc, dc_code)\n",
    "    cbdc_c = encode(cbdc, dc_code)\n",
    "    crdc_c = encode(crdc, dc_code)\n",
    "    # AC\n",
    "    yac_c = encode(yac, ac_code)\n",
    "    cbac_c = encode(cbac, ac_code)\n",
    "    crac_c = encode(crac, ac_code)\n",
    "    # Return \n",
    "    all_bits = (ydc_c, yac_c, cbdc_c, cbac_c, crdc_c, crac_c)\n",
    "    return all_bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load image\n",
    "img = Image.open(\"NetaLi_small.tiff\")\n",
    "# img = Image.open(\"creek.png\")\n",
    "# img = Image.open(\"glass.png\")\n",
    "img = np.array(img)[:,:,0:3]\n",
    "M, N = img.shape[0:2]\n",
    "\n",
    "# Display image\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(img), plt.xticks([]), plt.yticks([])\n",
    "plt.show()\n",
    "\n",
    "# Image storage size\n",
    "npixels = M * N\n",
    "nbits_raw = npixels * 8 * 3\n",
    "print(\"RGB{8,8,8} bits:\", nbits_raw);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-byte entropy\n",
    "img_rgb_freqs = calc_letter_frequencies(np.ravel(img))\n",
    "img_entropy = get_entropy(img_rgb_freqs)\n",
    "print(\"Image R/G/B entropy:\", img_entropy)\n",
    "print(\"Image bits per R/G/B value:\", nbits_raw / npixels / 3)\n",
    "plt.figure(figsize=(12,7))\n",
    "plt.bar(img_rgb_freqs.keys(), img_rgb_freqs.values())\n",
    "plt.title(\"R/G/B value frequencies\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RGB tuple entropy\n",
    "pixels = img.reshape(-1, 3)\n",
    "pixel_tuples = [(rgb[0], rgb[1], rgb[2]) for rgb in pixels]\n",
    "img_pixel_freqs = calc_letter_frequencies(pixel_tuples)\n",
    "img_pixel_entropy = get_entropy(img_pixel_freqs)\n",
    "print(\"Image pixel entropy:\", img_pixel_entropy)\n",
    "print(\"Image bpp:\", nbits_raw / npixels)\n",
    "plt.figure(figsize=(12,7))\n",
    "# plt.bar(list(map(str, img_pixel_freqs.keys())), img_pixel_freqs.values())\n",
    "plt.stem(img_pixel_freqs.values())\n",
    "plt.xticks([])\n",
    "plt.title(\"Pixel value frequencies\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the DC and AC codes\n",
    "It turns out that the DC and AC coefficients come from sufficiently different distributions that it is often more effective to compress them with separate codes than try to include both in a single code.\n",
    "\n",
    "Let's do that here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform compression up to Huffman encoding\n",
    "ydc, yac, cbdc, cbac, crdc, crac = process_image(img, quality=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the coefficient frequency tables\n",
    "dc_freqs = calc_letter_frequencies(ydc + cbdc + crdc)\n",
    "ac_freqs = calc_letter_frequencies(yac + cbac + crac)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the frequency distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "plt.bar(dc_freqs.keys(), dc_freqs.values())\n",
    "plt.title(\"DC coefficient probabilities\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "plt.bar(list(map(str, ac_freqs.keys())), ac_freqs.values())\n",
    "plt.title(\"AC coefficient probabilities\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can construct the two codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Huffman code dictionaries\n",
    "dc_code = get_code_dict(dc_freqs)\n",
    "ac_code = get_code_dict(ac_freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the entropies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"H(dc):\", get_entropy(dc_freqs))\n",
    "print(\"H(ac):\", get_entropy(ac_freqs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, we want to encode each of Y, Cb, Cr and AC, DC separately so that we can recover the blocks nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ydc_c = encode(ydc, dc_code)\n",
    "cbdc_c = encode(cbdc, dc_code)\n",
    "crdc_c = encode(crdc, dc_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Y DC bits:\", len(ydc_c))\n",
    "print(\"Cb DC bits:\", len(cbdc_c))\n",
    "print(\"Cr DC bits:\", len(crdc_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yac_c = encode(yac, ac_code)\n",
    "cbac_c = encode(cbac, ac_code)\n",
    "crac_c = encode(crac, ac_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Y AC bits:\", len(yac_c))\n",
    "print(\"Cb AC bits:\", len(cbac_c))\n",
    "print(\"Cr AC bits:\", len(crac_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_bits = (ydc_c, yac_c, cbdc_c, cbac_c, crdc_c, crac_c)\n",
    "nbits_custom = sum([len(b) for b in all_bits])\n",
    "print(\"Total bits:\", nbits_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our default Huffman table in Part 1 used **76172** bits to encode all coefficients, with a compression ratio of **18.15**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbits_default = 76172\n",
    "print(\"Custom compression ratio:\", nbits_default / nbits_custom)\n",
    "print(\"Total compression ratio:\", nbits_raw / nbits_custom)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the entropy of our new encoded bits? \n",
    "\n",
    "What do we want it to be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bit_freqs = calc_letter_frequencies(\"\".join(all_bits))\n",
    "bit_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hdist(binomial(bit_freqs[\"1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other quality factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default Huffman tables\n",
    "quality = 15\n",
    "bits = encode_image(img, quality=quality)\n",
    "num_bits = [len(b) for b in bits]\n",
    "num_pix = M * N\n",
    "img_dec = decode_image(bits, M, N, quality=quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(img), plt.xticks([]), plt.yticks([])\n",
    "plt.title(\"Original image\")\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(img_dec.astype(np.uint8)), plt.xticks([]), plt.yticks([])\n",
    "plt.title(\"Compressed image\")\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(abs((img_dec - img.astype(float))*10).astype(np.uint8)), plt.xticks([]), plt.yticks([])\n",
    "plt.title(\"Error image x 10\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_bits = custom_encode_image(img, quality=quality, plot_freqs=True)\n",
    "nbits = sum([len(b) for b in custom_bits])\n",
    "print(\"Extra compression ratio:\", sum(num_bits) / nbits)"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "interpreter": {
   "hash": "333cfc00ddea1a00d0bfe492c09bf38fb213b45d931eb893a4400886f172f537"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
